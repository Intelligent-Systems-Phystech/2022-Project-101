@article{litlink1,
      title={Unified Analysis of Stochastic Gradient Methods for Composite Convex and Smooth Optimization}, 
      author={Ahmed Khaled and Othmane Sebbouh and Nicolas Loizou and Robert M. Gower and Peter Richtárik},
      year={2020},
      eprint={2006.11573},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{litlink2,
      title={A Unified Theory of SGD: Variance Reduction, Sampling, Quantization and Coordinate Descent}, 
      author={Eduard Gorbunov and Filip Hanzely and Peter Richtárik},
      year={2019},
      eprint={1905.11261},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@article{litlink3,
    title={A Superlinearly-Convergent Proximal Newton-type Method for the Optimization of Finite Sums},
    author={Anton Rodomanov and Dmitry Kropotov},
    booktitle={ICML},
    year={2016}
}

@article{litlink4,
  author    = {Dmitry Kovalev and
               Konstantin Mishchenko and
               Peter Richt{\'{a}}rik},
  title     = {Stochastic Newton and Cubic Newton Methods with Simple Local Linear-Quadratic
               Rates},
  journal   = {CoRR},
  volume    = {abs/1912.01597},
  year      = {2019},
  url       = {http://arxiv.org/abs/1912.01597},
  eprinttype = {arXiv},
  eprint    = {1912.01597},
  timestamp = {Thu, 02 Jan 2020 18:08:18 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1912-01597.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{litlink5,
      title={Parallel Coordinate Descent Methods for Big Data Optimization}, 
      author={Peter Richtárik and Martin Takáč},
      year={2013},
      eprint={1212.0873},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@misc{litlink6,
      title={On Optimal Probabilities in Stochastic Coordinate Descent Methods}, 
      author={Peter Richtárik and Martin Takáč},
      year={2013},
      eprint={1310.3438},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{litlink7,
      title={Exact and Inexact Subsampled Newton Methods for Optimization}, 
      author={Raghu Bollapragada and Richard Byrd and Jorge Nocedal},
      year={2016},
      eprint={1609.08502},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}


@article{svrg,
  author = {Rie Johnson and Tong Zhang},
  year = {2013},
  title = { Accelerating stochastic gradient descent using predictive variance reduction, Advances in Neural Information Processing Systems},
} 


@article{saga,
  author = {M. Schmidt and N. Le Roux and F. Bach},
  year = {2017},
  title = {Minimizing finite sums with the stochastic average gradient},
    journal = {Math. Program.},
     volume    = {162},
  number    = {1-2},
  pages     = {83--112},
  year      = {2017}
} 
